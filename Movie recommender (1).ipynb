{"cells":[{"metadata":{},"cell_type":"markdown","source":"**NB : Ce code doit etre importé et exécuté sur Kaggle.com, il génére des erreurs sur les autres plateformes (comme Jupyter, Spider ...)**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.sql(\"select 'spark' as hello\")\n\ndf.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"#genome_scores = pd.read_csv(\"../input/movielens-20m-dataset/genome_scores.csv\")\n#genome_tags = pd.read_csv(\"../input/movielens-20m-dataset/genome_tags.csv\")\n#link = pd.read_csv(\"../input/movielens-20m-dataset/link.csv\")\n#movie = pd.read_csv(\"../input/movielens-20m-dataset/movie.csv\")\n#rating = pd.read_csv(\"../input/movielens-20m-dataset/rating.csv\")\n#tag = pd.read_csv(\"../input/movielens-20m-dataset/tag.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# La table des notes des films\nmovie_ratings = spark.read.csv(\"../input/movielens-20m-dataset/rating.csv\", inferSchema=True, header=True)\nmovie_ratings = movie_ratings.select('userId', 'movieId', 'rating')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(movie_ratings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cette contient environs 20 milions de lignes, on prend juste les 1 milions premiers\nmovie_ratings = movie_ratings.head(1000000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(movie_ratings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# l'étape precedante nous retourne une liste, on crée une liste des noms des colonnes avec lesquels on vas structurer la liste movie_ratings\ncolumns = list(['userId', 'movieId', 'rating'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Création d'une dataframe à partir de la liste movie_ratings avec les noms de colonnes columns\nmovie_ratings = spark.createDataFrame(movie_ratings, columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_ratings.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movie_ratings.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Alternating Least Squares (Alternance des moindres carrés)\nfrom pyspark.ml.recommendation import ALS\n# RegressionEvaluator pour évaluer la performance du modèle ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\n# CrossValidator pour diviser la dataset en training and testing\n# ParamGridBuilder pour affiner les paramètres de notre modèle\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Création de training set et test set\n(training, test) = movie_ratings.randomSplit([0.8, 0.2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Création du modèle ALS (Alternating Least Saqures)\nals = ALS(userCol='userId', itemCol='movieId', ratingCol='rating', coldStartStrategy='drop', nonnegative=True)\n\n# nonnegative=True : car on veut pas qu'il nous retourne des valeurs négatives","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Régler le modèle en utilisant ParamGridBuilder\nparam_grid = ParamGridBuilder().addGrid(als.rank, [12, 13, 14]).addGrid(als.maxIter, [18, 19, 20]).addGrid(als.regParam, [.17, .18, .19]).build()\n\n# On le donne :\n# les paramètres des matrices U et P\n# max iterations qui disent à Spark combien de fois alterner entre U et P pour minimiser l'erreur\n# le paramètre de régularisation pour empêcher ALS de sur-adapter aux données (overfitting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Définir l'Évaluateur de régression, qui attend la prédiction des colonnes d'entrée\nevaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n\n# predictionCol='prediction' : le nom de la colonne des prédictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construction de cross validation\ncv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n\n# estimator=als : pour utiliser le modèle de ALS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Entrainer le modèle avec les données d'entraînement\nmodel = cv.fit(training)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extraire le meilleur modèle de l'exercice de tournage à l'aide de ParamGridBuilder\nbest_model = model.bestModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Générer des prédictions et évaluer à l'aide de RMSE\npredictions = best_model.transform(test)\nrmse = evaluator.evaluate(predictions)\n\n# rmse : Écart quadratique moyen (Root-mean-square deviation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Afficher les métriques d'évaluation du modèle\nprint(\"RMSE = \" + str(rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Comparer les prédictions des évaluations des utilisateurs (ratings) avec les évaluations réels\npredictions.sort('userId', 'rating').show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Génerer les prédictions des évaluations de tous les utilisateurs (Recommander 10 films pour chaque utilisateur)\nusers_recommendations = best_model.recommendForAllUsers(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users_recommendations.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SQLContext : Le point d'entrée pour travailler avec des données structurées (lignes et colonnes) dans Spark\nfrom pyspark.sql import SQLContext\n\nsqlContext = SQLContext(spark)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ajout de la table des films pour joindre les noms des films avec la table de recommendations en résultat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movieDF = spark.read.csv(\"../input/movielens-20m-dataset/movie.csv\", inferSchema=True, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movieDF.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pour faciliter l'affichage de users_recommendations\n\ndef get_recs_for_user(recs):\n    recs = recs.select(\"userId\", \"recommendations.movieId\", \"recommendations.rating\")\n    movies = recs.select(\"movieId\").toPandas().iloc[:, 0].values\n    ratings = recs.select(\"rating\").toPandas().iloc[:, 0].values\n    userIds = recs.select(\"userId\").toPandas()\n    ratings_matrix = pd.DataFrame(movies, columns=['movieId'])\n    #ratings_matrix['userId'] = userIds\n    ratings_matrix.insert(0, 'userId', userIds)\n    ratings_matrix['ratings'] = ratings\n    ratings_matrix_ps = sqlContext.createDataFrame(ratings_matrix)\n    return ratings_matrix_ps","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users_recs = get_recs_for_user(users_recommendations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(users_recs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users_recs.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Extraire de users_recommendations les recommandations pour un utilisateur spécifique\nuser_id = input(\"Donner l'id de l'utilisateur : \")\nuser_recs = users_recs.filter(\"userId=\"+user_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"user_recs.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pour changer l'affichage de la liste des ids des films\n\nz = []\n\nfor k,row in user_recs.toPandas().iterrows():\n    for j in list(np.array(row.movieId).flat):\n        z.append({'userId':row.userId, 'movieId':j})\n\nuser_recs = spark.createDataFrame(pd.DataFrame(z))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Joindre la dataframe des films recommandés pour l'utilisateur avec leurs titres et genres\n\nuser_recs = user_recs.join(movieDF, on='movieId')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pour échanger les indexs de 'userId' et 'movieId'\n\nuser_recs = user_recs['userId', 'movieId', 'title', 'genres']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Affichage de la table de recommendation pour l'utilisateur demandé\nuser_recs.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}